{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8790689a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTI-NLP Enhanced Threat Analyzer - Model Training v2.0\n",
      "This notebook trains TWO models:\n",
      "  1. CTI Report Classifier (Sentiment + Severity)\n",
      "  2. URL Threat Detector (55 features)\n",
      "\n",
      "Starting training process...\n",
      "This will take approximately 5-10 minutes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CTI-NLP Enhanced Analyzer - Model Training Notebook\n",
    "print(\"CTI-NLP Enhanced Threat Analyzer - Model Training v2.0\")\n",
    "print(\"This notebook trains TWO models:\")\n",
    "print(\"  1. CTI Report Classifier (Sentiment + Severity)\")\n",
    "print(\"  2. URL Threat Detector (55 features)\")\n",
    "\n",
    "print(\"\\nStarting training process...\")\n",
    "print(\"This will take approximately 5-10 minutes.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7541196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CELL 2: Importing Required Libraries\n",
      "============================================================\n",
      "✓ All libraries imported successfully\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Import Libraries\n",
    "print(\"=\" * 60)\n",
    "print(\"CELL 2: Importing Required Libraries\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "from datetime import datetime\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    classification_report, \n",
    "    confusion_matrix,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ All libraries imported successfully\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3baa2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CELL 3: Configuration Settings\n",
      "============================================================\n",
      "✓ Configuration loaded\n",
      "  CTI Files: 5\n",
      "  URL Dataset: data/url_dataset.csv\n",
      "  Trusted Domains: 26\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Configuration\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CELL 3: Configuration Settings\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# CTI Report Configuration\n",
    "CTI_FILE_PATHS = [\n",
    "    'data/Cybersecurity_Dataset.csv',\n",
    "    'data/cyber-threat-intelligence-all.csv',\n",
    "    'data/cyber-threat-intelligence-splited_train.csv',\n",
    "    'data/cyber-threat-intelligence-splited_test.csv',\n",
    "    'data/cyber-threat-intelligence-splited_val.csv'\n",
    "]\n",
    "CTI_LABEL_COLUMN = 'Threat Category'\n",
    "CTI_FEATURE_COLS = ['Sentiment in Forums', 'Severity Score']\n",
    "\n",
    "# URL Configuration\n",
    "URL_FILE_PATH = 'data/url_dataset.csv'\n",
    "URL_LABEL_COLUMN = 'type'\n",
    "URL_INPUT_COLUMN = 'url'\n",
    "\n",
    "# Trusted Domains for URL Analysis\n",
    "TRUSTED_DOMAINS = {\n",
    "    'google.com', 'youtube.com', 'facebook.com', 'amazon.com', 'wikipedia.org',\n",
    "    'twitter.com', 'instagram.com', 'linkedin.com', 'reddit.com', 'github.com',\n",
    "    'microsoft.com', 'apple.com', 'netflix.com', 'yahoo.com', 'bing.com',\n",
    "    'stackoverflow.com', 'medium.com', 'dropbox.com', 'adobe.com', 'paypal.com',\n",
    "    'ebay.com', 'cnn.com', 'bbc.com', 'nytimes.com', 'spotify.com','paypal.com','https://geethashishu.in/'\n",
    "}\n",
    "\n",
    "LEGITIMATE_TLDS = {'.com', '.org', '.net', '.edu', '.gov', '.co', '.io', '.ai', '.in'}\n",
    "SUSPICIOUS_TLDS = {'.tk', '.ml', '.ga', '.cf', '.gq', '.zip', '.review', '.xyz', '.top'}\n",
    "\n",
    "print(\"✓ Configuration loaded\")\n",
    "print(f\"  CTI Files: {len(CTI_FILE_PATHS)}\")\n",
    "print(f\"  URL Dataset: {URL_FILE_PATH}\")\n",
    "print(f\"  Trusted Domains: {len(TRUSTED_DOMAINS)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7ff4108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CELL 4: Loading CTI Report Training Data\n",
      "============================================================\n",
      "✓ Loaded: data/Cybersecurity_Dataset.csv (1100 records)\n",
      "⏭  Skipped: data/cyber-threat-intelligence-all.csv (not found)\n",
      "⚠  Error: data/cyber-threat-intelligence-splited_train.csv - Column mismatch\n",
      "⚠  Error: data/cyber-threat-intelligence-splited_test.csv - Column mismatch\n",
      "⏭  Skipped: data/cyber-threat-intelligence-splited_val.csv (not found)\n",
      "\n",
      "✓ CTI Data Loaded: 686 records after cleaning\n",
      "  Features: ['Sentiment in Forums', 'Severity Score']\n",
      "  Classes: ['DDoS' 'Malware' 'Phishing' 'Ransomware']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load CTI Report Data\n",
    "print(\"=\" * 60)\n",
    "print(\"CELL 4: Loading CTI Report Training Data\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_dfs = []\n",
    "for path in CTI_FILE_PATHS:\n",
    "    try:\n",
    "        df_part = pd.read_csv(path, usecols=[CTI_LABEL_COLUMN] + CTI_FEATURE_COLS)\n",
    "        all_dfs.append(df_part)\n",
    "        print(f\"✓ Loaded: {path} ({len(df_part)} records)\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"⏭  Skipped: {path} (not found)\")\n",
    "    except ValueError as e:\n",
    "        print(f\"⚠  Error: {path} - Column mismatch\")\n",
    "        continue\n",
    "\n",
    "if not all_dfs:\n",
    "    print(\"\\n⚠ WARNING: No CTI files loaded. Using simulated data.\")\n",
    "    data = {\n",
    "        'Threat Category': ['DDoS', 'Malware', 'Phishing', 'Ransomware', 'Benign'] * 20,\n",
    "        'Sentiment in Forums': np.random.uniform(0.1, 0.9, 100),\n",
    "        'Severity Score': np.random.randint(1, 6, 100)\n",
    "    }\n",
    "    df_cti = pd.DataFrame(data)\n",
    "else:\n",
    "    df_cti = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "# Clean data\n",
    "for col in CTI_FEATURE_COLS:\n",
    "    df_cti[col] = pd.to_numeric(df_cti[col], errors='coerce')\n",
    "\n",
    "df_cti.dropna(subset=[CTI_LABEL_COLUMN] + CTI_FEATURE_COLS, inplace=True)\n",
    "df_cti.drop_duplicates(inplace=True)\n",
    "\n",
    "print(f\"\\n✓ CTI Data Loaded: {len(df_cti)} records after cleaning\")\n",
    "print(f\"  Features: {CTI_FEATURE_COLS}\")\n",
    "print(f\"  Classes: {df_cti[CTI_LABEL_COLUMN].unique()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79ea5d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CELL 5: Defining URL Feature Extraction\n",
      "============================================================\n",
      "✓ URL feature extraction function defined\n",
      "  Total features: 55\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#URL Feature Extraction Function\n",
    "print(\"=\" * 60)\n",
    "print(\"CELL 5: Defining URL Feature Extraction\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def extract_url_features(url):\n",
    "    \"\"\"\n",
    "    Extracts 55 comprehensive features from a URL for threat detection.\n",
    "    This MUST match the features in app.py!\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        parsed = urlparse(url.strip())\n",
    "        domain = parsed.netloc.lower() if parsed.netloc else \"\"\n",
    "        path = parsed.path if parsed.path else \"\"\n",
    "        query = parsed.query if parsed.query else \"\"\n",
    "        scheme = parsed.scheme if parsed.scheme else \"\"\n",
    "    except:\n",
    "        domain = path = query = scheme = \"\"\n",
    "    \n",
    "    # Clean domain\n",
    "    clean_domain = re.sub(r':\\d+$', '', domain)\n",
    "    clean_domain = re.sub(r'^www\\.', '', clean_domain)\n",
    "    \n",
    "    #TRUST INDICATORS\n",
    "    features['is_trusted_domain'] = 1 if any(trusted in clean_domain for trusted in TRUSTED_DOMAINS) else 0\n",
    "    features['has_legitimate_tld'] = 1 if any(clean_domain.endswith(tld) for tld in LEGITIMATE_TLDS) else 0\n",
    "    features['has_suspicious_tld'] = 1 if any(clean_domain.endswith(tld) for tld in SUSPICIOUS_TLDS) else 0\n",
    "    \n",
    "    #LENGTH FEATURES\n",
    "    features['url_length'] = len(url)\n",
    "    features['domain_length'] = len(domain)\n",
    "    features['path_length'] = len(path)\n",
    "    features['query_length'] = len(query)\n",
    "    \n",
    "    #CHARACTER COUNTS\n",
    "    features['num_dots'] = url.count('.')\n",
    "    features['num_hyphens'] = url.count('-')\n",
    "    features['num_underscores'] = url.count('_')\n",
    "    features['num_slashes'] = url.count('/')\n",
    "    features['num_at_symbol'] = url.count('@')\n",
    "    features['num_question_mark'] = url.count('?')\n",
    "    features['num_ampersand'] = url.count('&')\n",
    "    features['num_equals'] = url.count('=')\n",
    "    features['num_percent'] = url.count('%')\n",
    "    features['num_digits'] = sum(c.isdigit() for c in url)\n",
    "    features['num_letters'] = sum(c.isalpha() for c in url)\n",
    "    features['num_special_chars'] = sum(not c.isalnum() and c not in './-_:?' for c in url)\n",
    "    \n",
    "    #DOMAIN ANALYSIS \n",
    "    features['num_dots_domain'] = domain.count('.')\n",
    "    features['num_hyphens_domain'] = domain.count('-')\n",
    "    features['has_ip_address'] = 1 if re.search(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', url) else 0\n",
    "    features['subdomain_count'] = max(0, len(domain.split('.')) - 2) if domain else 0\n",
    "    features['domain_has_digits'] = 1 if any(c.isdigit() for c in domain) else 0\n",
    "    \n",
    "    #SECURITY FEATUREs\n",
    "    features['has_https'] = 1 if scheme == 'https' else 0\n",
    "    features['has_http'] = 1 if scheme == 'http' else 0\n",
    "    features['port_in_url'] = 1 if re.search(r':\\d{2,5}', domain) else 0\n",
    "    \n",
    "    #PHISHING INDICATORS\n",
    "    features['excessive_dots_in_path'] = 1 if path.count('..') > 0 or path.count('...') > 0 else 0\n",
    "    features['has_wp_includes'] = 1 if 'wp-includes' in url.lower() or 'wp-admin' in url.lower() else 0\n",
    "    features['has_admin_path'] = 1 if '/admin' in url.lower() or '/administrator' in url.lower() else 0\n",
    "    \n",
    "    phishing_keywords = ['login', 'signin', 'verify', 'update', 'secure', 'account', \n",
    "                         'banking', 'confirm', 'suspended', 'locked', 'paypal', 'dropbox']\n",
    "    features['phishing_keyword_count'] = sum(1 for kw in phishing_keywords if kw in url.lower())\n",
    "    \n",
    "    trusted_brands = ['google', 'paypal', 'amazon', 'microsoft', 'apple', 'facebook', 'dropbox']\n",
    "    features['brand_impersonation'] = 1 if any(brand in clean_domain for brand in trusted_brands) and clean_domain not in TRUSTED_DOMAINS else 0\n",
    "    \n",
    "    features['has_php_file'] = 1 if '.php' in path.lower() else 0\n",
    "    features['has_html_file'] = 1 if '.htm' in path.lower() or '.html' in path.lower() else 0\n",
    "    \n",
    "    #ENTROPY (Randomness)\n",
    "    def calculate_entropy(text):\n",
    "        if not text or len(text) < 2:\n",
    "            return 0\n",
    "        prob = [float(text.count(c)) / len(text) for c in set(text)]\n",
    "        return -sum(p * np.log2(p) for p in prob if p > 0)\n",
    "    \n",
    "    features['url_entropy'] = calculate_entropy(url)\n",
    "    features['domain_entropy'] = calculate_entropy(domain)\n",
    "    features['path_entropy'] = calculate_entropy(path) if path else 0\n",
    "    features['high_domain_entropy'] = 1 if features['domain_entropy'] > 4.0 else 0\n",
    "    features['high_path_entropy'] = 1 if features['path_entropy'] > 4.5 else 0\n",
    "    \n",
    "    #RATIO FEATURES\n",
    "    url_len = max(len(url), 1)\n",
    "    features['digit_ratio'] = features['num_digits'] / url_len\n",
    "    features['special_char_ratio'] = features['num_special_chars'] / url_len\n",
    "    features['dots_to_length_ratio'] = features['num_dots'] / url_len\n",
    "    \n",
    "    #SUSPICIOUS PATTERNS\n",
    "    features['excessive_subdomains'] = 1 if features['subdomain_count'] > 3 else 0\n",
    "    features['url_shortener'] = 1 if any(s in clean_domain for s in ['bit.ly', 'tinyurl', 'goo.gl', 't.co']) else 0\n",
    "    features['hyphen_in_domain'] = 1 if '-' in clean_domain else 0\n",
    "    \n",
    "    path_parts = [p for p in path.split('/') if p and len(p) > 10]\n",
    "    features['has_long_random_path'] = 1 if any(len(p) > 32 for p in path_parts) else 0\n",
    "    \n",
    "    #PATH ANALYSIS\n",
    "    features['path_depth'] = len([p for p in path.split('/') if p])\n",
    "    features['deep_path'] = 1 if features['path_depth'] > 5 else 0\n",
    "    \n",
    "    features['has_hex_encoding'] = 1 if re.search(r'%[0-9a-fA-F]{2}', url) else 0\n",
    "    features['has_at_symbol'] = 1 if '@' in url else 0\n",
    "    features['has_double_slash'] = 1 if '//' in path else 0\n",
    "    \n",
    "    #QUERY PARAMETERS\n",
    "    features['num_query_params'] = query.count('&') + (1 if query else 0)\n",
    "    features['has_redirect'] = 1 if any(p in query.lower() for p in ['redirect', 'url=', 'next=', 'goto=']) else 0\n",
    "    \n",
    "    #DOMAIN STRUCTURE\n",
    "    if domain:\n",
    "        parts = domain.split('.')\n",
    "        if len(parts) >= 2:\n",
    "            features['main_domain_length'] = len(parts[-2])\n",
    "            features['tld_length'] = len(parts[-1])\n",
    "        else:\n",
    "            features['main_domain_length'] = 0\n",
    "            features['tld_length'] = 0\n",
    "    else:\n",
    "        features['main_domain_length'] = 0\n",
    "        features['tld_length'] = 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "print(\"✓ URL feature extraction function defined\")\n",
    "print(\"  Total features: 55\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84d79126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CELL 6: Loading URL Training Data\n",
      "============================================================\n",
      "✓ Loaded: data/url_dataset.csv\n",
      "  Total records: 450176\n",
      "  After cleaning: 450176 records (0 removed)\n",
      "\n",
      "  Label distribution:\n",
      "    - legitimate: 345738 (76.8%)\n",
      "    - phishing: 104438 (23.2%)\n"
     ]
    }
   ],
   "source": [
    "#Load URL Data\n",
    "print(\"=\" * 60)\n",
    "print(\"CELL 6: Loading URL Training Data\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    df_url = pd.read_csv(URL_FILE_PATH)\n",
    "    print(f\"✓ Loaded: {URL_FILE_PATH}\")\n",
    "    print(f\"  Total records: {len(df_url)}\")\n",
    "    \n",
    "    # Check columns\n",
    "    if URL_INPUT_COLUMN not in df_url.columns:\n",
    "        print(f\"\\n⚠ Warning: Column '{URL_INPUT_COLUMN}' not found\")\n",
    "        print(f\"  Available columns: {list(df_url.columns)}\")\n",
    "        # Try to find URL column\n",
    "        url_cols = [col for col in df_url.columns if 'url' in col.lower()]\n",
    "        if url_cols:\n",
    "            URL_INPUT_COLUMN = url_cols[0]\n",
    "            print(f\"  Using: {URL_INPUT_COLUMN}\")\n",
    "    \n",
    "    if URL_LABEL_COLUMN not in df_url.columns:\n",
    "        label_cols = [col for col in df_url.columns if col != URL_INPUT_COLUMN]\n",
    "        if label_cols:\n",
    "            URL_LABEL_COLUMN = label_cols[0]\n",
    "            print(f\"  Using label column: {URL_LABEL_COLUMN}\")\n",
    "    \n",
    "    # Clean data\n",
    "    initial_count = len(df_url)\n",
    "    df_url.dropna(subset=[URL_INPUT_COLUMN], inplace=True)\n",
    "    df_url[URL_INPUT_COLUMN] = df_url[URL_INPUT_COLUMN].str.strip()\n",
    "    df_url.drop_duplicates(subset=[URL_INPUT_COLUMN], inplace=True)\n",
    "    \n",
    "    print(f\"  After cleaning: {len(df_url)} records ({initial_count - len(df_url)} removed)\")\n",
    "    print(f\"\\n  Label distribution:\")\n",
    "    for label, count in df_url[URL_LABEL_COLUMN].value_counts().items():\n",
    "        print(f\"    - {label}: {count} ({count/len(df_url)*100:.1f}%)\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"⚠ ERROR: {URL_FILE_PATH} not found!\")\n",
    "    print(\"  Creating sample dataset...\")\n",
    "    \n",
    "    # Create sample dataset\n",
    "    sample_urls = {\n",
    "        'legitimate': [\n",
    "            'https://www.google.com',\n",
    "            'https://www.facebook.com',\n",
    "            'https://github.com/user/repo',\n",
    "            'https://www.amazon.com/product',\n",
    "            'https://stackoverflow.com/questions/123',\n",
    "        ] * 100,\n",
    "        'phishing': [\n",
    "            'http://secure-paypal-login.tk',\n",
    "            'http://verify-account-apple.ml',\n",
    "            'http://update-banking-info.ga',\n",
    "            'http://suspended-account-fix.cf',\n",
    "            'http://bit.ly/malware123',\n",
    "        ] * 100\n",
    "    }\n",
    "    \n",
    "    urls = []\n",
    "    labels = []\n",
    "    for label, url_list in sample_urls.items():\n",
    "        urls.extend(url_list)\n",
    "        labels.extend([label] * len(url_list))\n",
    "    \n",
    "    df_url = pd.DataFrame({URL_INPUT_COLUMN: urls, URL_LABEL_COLUMN: labels})\n",
    "    print(f\"✓ Created sample dataset: {len(df_url)} records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69ac81cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CELL 7: Extracting URL Features\n",
      "============================================================\n",
      "This may take a few minutes for large datasets\n",
      "\n",
      "✓ Feature extraction complete\n",
      "  Samples processed: 450176\n",
      "  Features extracted: 55\n",
      "  Feature names: ['is_trusted_domain', 'has_legitimate_tld', 'has_suspicious_tld', 'url_length', 'domain_length', 'path_length', 'query_length', 'num_dots', 'num_hyphens', 'num_underscores']... (showing first 10)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Extract URL Features\n",
    "print(\"=\" * 60)\n",
    "print(\"CELL 7: Extracting URL Features\")\n",
    "print(\"=\" * 60)\n",
    "print(\"This may take a few minutes for large datasets\")\n",
    "\n",
    "url_features = df_url[URL_INPUT_COLUMN].apply(lambda x: pd.Series(extract_url_features(x)))\n",
    "\n",
    "print(f\"\\n✓ Feature extraction complete\")\n",
    "print(f\"  Samples processed: {len(url_features)}\")\n",
    "print(f\"  Features extracted: {len(url_features.columns)}\")\n",
    "print(f\"  Feature names: {list(url_features.columns[:10])}... (showing first 10)\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94b0447c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CELL 8: Training CTI Report Classifier\n",
      "============================================================\n",
      "\n",
      "✓ CTI Model Training Complete\n",
      "  Training samples: 480\n",
      "  Test samples: 206\n",
      "  Accuracy: 0.2184 (21.84%)\n",
      "  Classes: ['DDoS', 'Malware', 'Phishing', 'Ransomware']\n",
      "\n",
      "✓ Saved:\n",
      "  - model.pkl\n",
      "  - feature_list.pkl\n",
      "  - threat_encoder.pkl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Train CTI Report Model\n",
    "print(\"=\" * 60)\n",
    "print(\"CELL 8: Training CTI Report Classifier\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Prepare CTI data\n",
    "X_cti = df_cti[CTI_FEATURE_COLS]\n",
    "y_cti = df_cti[CTI_LABEL_COLUMN].astype(str)\n",
    "\n",
    "# Encode labels\n",
    "cti_encoder = LabelEncoder()\n",
    "y_cti_encoded = cti_encoder.fit_transform(y_cti)\n",
    "\n",
    "# Split data\n",
    "X_cti_train, X_cti_test, y_cti_train, y_cti_test = train_test_split(\n",
    "    X_cti, y_cti_encoded, test_size=0.3, random_state=42, stratify=y_cti_encoded\n",
    ")\n",
    "\n",
    "# Train model\n",
    "cti_model = LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=1000, random_state=42)\n",
    "cti_model.fit(X_cti_train, y_cti_train)\n",
    "\n",
    "# Evaluate\n",
    "y_cti_pred = cti_model.predict(X_cti_test)\n",
    "cti_accuracy = accuracy_score(y_cti_test, y_cti_pred)\n",
    "\n",
    "print(f\"\\n✓ CTI Model Training Complete\")\n",
    "print(f\"  Training samples: {len(X_cti_train)}\")\n",
    "print(f\"  Test samples: {len(X_cti_test)}\")\n",
    "print(f\"  Accuracy: {cti_accuracy:.4f} ({cti_accuracy*100:.2f}%)\")\n",
    "print(f\"  Classes: {list(cti_encoder.classes_)}\")\n",
    "\n",
    "# Save CTI model\n",
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(cti_model, f)\n",
    "with open('feature_list.pkl', 'wb') as f:\n",
    "    pickle.dump(CTI_FEATURE_COLS, f)\n",
    "with open('threat_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(cti_encoder, f)\n",
    "\n",
    "print(f\"\\n✓ Saved:\")\n",
    "print(f\"  - model.pkl\")\n",
    "print(f\"  - feature_list.pkl\")\n",
    "print(f\"  - threat_encoder.pkl\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95be1525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CELL 9: Training URL Threat Detector\n",
      "============================================================\n",
      "Training set: 360140 samples\n",
      "Test set: 90036 samples\n",
      "\n",
      "Training Random Forest... (this may take 1-2 minutes)\n",
      "\n",
      "✓ URL Model Training Complete\n",
      "  Training Accuracy: 0.9975 (99.75%)\n",
      "  Test Accuracy: 0.9964 (99.64%)\n",
      "  OOB Score: 0.9963 (99.63%)\n",
      "  Classes: ['legitimate', 'phishing']\n",
      "\n",
      "✓ Saved:\n",
      "  - url_model.pkl\n",
      "  - url_feature_names.pkl\n",
      "  - url_label_encoder.pkl\n",
      "  - url_trusted_domains.pkl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Train URL Threat Model\n",
    "print(\"=\" * 60)\n",
    "print(\"CELL 9: Training URL Threat Detector\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Prepare URL data\n",
    "X_url = url_features\n",
    "y_url = df_url[URL_LABEL_COLUMN]\n",
    "\n",
    "# Encode labels\n",
    "y_url_encoded, url_label_encoder = pd.factorize(y_url)\n",
    "url_feature_names = list(X_url.columns)\n",
    "\n",
    "# Split data\n",
    "X_url_train, X_url_test, y_url_train, y_url_test = train_test_split(\n",
    "    X_url, y_url_encoded, test_size=0.2, random_state=42, stratify=y_url_encoded\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_url_train)} samples\")\n",
    "print(f\"Test set: {len(X_url_test)} samples\")\n",
    "\n",
    "# Train Random Forest model\n",
    "url_model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=20,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    max_features='sqrt',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced',\n",
    "    bootstrap=True,\n",
    "    oob_score=True\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Random Forest... (this may take 1-2 minutes)\")\n",
    "url_model.fit(X_url_train, y_url_train)\n",
    "\n",
    "# Evaluate\n",
    "train_acc = url_model.score(X_url_train, y_url_train)\n",
    "test_acc = url_model.score(X_url_test, y_url_test)\n",
    "oob_score = url_model.oob_score_ if hasattr(url_model, 'oob_score_') else 0\n",
    "\n",
    "print(f\"\\n✓ URL Model Training Complete\")\n",
    "print(f\"  Training Accuracy: {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
    "print(f\"  Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "print(f\"  OOB Score: {oob_score:.4f} ({oob_score*100:.2f}%)\")\n",
    "print(f\"  Classes: {list(url_label_encoder)}\")\n",
    "\n",
    "# Save URL model\n",
    "with open('url_model.pkl', 'wb') as f:\n",
    "    pickle.dump(url_model, f)\n",
    "with open('url_feature_names.pkl', 'wb') as f:\n",
    "    pickle.dump(url_feature_names, f)\n",
    "with open('url_label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(url_label_encoder, f)\n",
    "with open('url_trusted_domains.pkl', 'wb') as f:\n",
    "    pickle.dump(TRUSTED_DOMAINS, f)\n",
    "\n",
    "print(f\"\\n✓ Saved:\")\n",
    "print(f\"  - url_model.pkl\")\n",
    "print(f\"  - url_feature_names.pkl\")\n",
    "print(f\"  - url_label_encoder.pkl\")\n",
    "print(f\"  - url_trusted_domains.pkl\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dd12258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CELL 10: URL Model - Detailed Evaluation\n",
      "============================================================\n",
      "\n",
      "Classification Report:\n",
      "------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  legitimate       1.00      1.00      1.00     69148\n",
      "    phishing       1.00      0.99      0.99     20888\n",
      "\n",
      "    accuracy                           1.00     90036\n",
      "   macro avg       1.00      0.99      0.99     90036\n",
      "weighted avg       1.00      1.00      1.00     90036\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "------------------------------------------------------------\n",
      "[[69097    51]\n",
      " [  271 20617]]\n",
      "\n",
      "Rows = Actual | Columns = Predicted\n",
      "Labels: ['legitimate', 'phishing']\n",
      "\n",
      "Top 15 Most Important Features:\n",
      "------------------------------------------------------------\n",
      " 1. has_https                      0.3344 █████████████████████████████████\n",
      " 2. has_http                       0.2716 ███████████████████████████\n",
      " 3. subdomain_count                0.0971 █████████\n",
      " 4. num_dots_domain                0.0765 ███████\n",
      " 5. has_legitimate_tld             0.0308 ███\n",
      " 6. num_dots                       0.0275 ██\n",
      " 7. phishing_keyword_count         0.0258 ██\n",
      " 8. tld_length                     0.0201 ██\n",
      " 9. brand_impersonation            0.0105 █\n",
      "10. num_digits                     0.0102 █\n",
      "11. main_domain_length             0.0097 \n",
      "12. has_php_file                   0.0089 \n",
      "13. domain_has_digits              0.0085 \n",
      "14. domain_length                  0.0072 \n",
      "15. num_hyphens                    0.0070 \n",
      "\n",
      "✓ Saved: url_feature_importance.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Detailed Evaluation - URL Model\n",
    "print(\"=\" * 60)\n",
    "print(\"CELL 10: URL Model - Detailed Evaluation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "y_url_pred = url_model.predict(X_url_test)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"-\" * 60)\n",
    "print(classification_report(y_url_test, y_url_pred, target_names=url_label_encoder))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(\"-\" * 60)\n",
    "cm = confusion_matrix(y_url_test, y_url_pred)\n",
    "print(cm)\n",
    "print(f\"\\nRows = Actual | Columns = Predicted\")\n",
    "print(f\"Labels: {list(url_label_encoder)}\")\n",
    "\n",
    "# Feature importance\n",
    "print(\"\\nTop 15 Most Important Features:\")\n",
    "print(\"-\" * 60)\n",
    "feature_importance = sorted(\n",
    "    zip(url_feature_names, url_model.feature_importances_),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "for i, (feature, importance) in enumerate(feature_importance[:15], 1):\n",
    "    bar = '█' * int(importance * 100)\n",
    "    print(f\"{i:2d}. {feature:30s} {importance:.4f} {bar}\")\n",
    "\n",
    "# Save feature importance\n",
    "importance_df = pd.DataFrame(feature_importance, columns=['Feature', 'Importance'])\n",
    "importance_df.to_csv('url_feature_importance.csv', index=False)\n",
    "print(f\"\\n✓ Saved: url_feature_importance.csv\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dff2ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CELL 11: Testing Predictions\n",
      "============================================================\n",
      "\n",
      "1. CTI Report Model Test:\n",
      "------------------------------------------------------------\n",
      "  Test 1: Sentiment=0.9, Severity=1\n",
      "    → Predicted: Malware (27.0% confidence)\n",
      "    → Expected: Low Risk\n",
      "  Test 2: Sentiment=0.5, Severity=3\n",
      "    → Predicted: DDoS (27.1% confidence)\n",
      "    → Expected: Medium Risk\n",
      "  Test 3: Sentiment=0.2, Severity=5\n",
      "    → Predicted: DDoS (30.8% confidence)\n",
      "    → Expected: High Risk\n",
      "\n",
      "2. URL Threat Model Test:\n",
      "------------------------------------------------------------\n",
      "  Test 1: https://www.google.com...\n",
      "    → Predicted: legitimate (97.8% confidence)\n",
      "    → Expected: Safe\n",
      "  Test 2: http://secure-paypal-verify.tk...\n",
      "    → Predicted: phishing (100.0% confidence)\n",
      "    → Expected: Phishing\n",
      "  Test 3: https://github.com/user/repo...\n",
      "    → Predicted: phishing (93.9% confidence)\n",
      "    → Expected: Safe\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Test Predictions\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CELL 11: Testing Predictions\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test CTI model\n",
    "print(\"\\n1. CTI Report Model Test:\")\n",
    "print(\"-\" * 60)\n",
    "test_cti_samples = [\n",
    "    {'sentiment': 0.9, 'severity': 1, 'expected': 'Low Risk'},\n",
    "    {'sentiment': 0.5, 'severity': 3, 'expected': 'Medium Risk'},\n",
    "    {'sentiment': 0.2, 'severity': 5, 'expected': 'High Risk'}\n",
    "]\n",
    "\n",
    "for i, sample in enumerate(test_cti_samples, 1):\n",
    "    input_data = np.array([[sample['sentiment'], sample['severity']]])\n",
    "    input_df = pd.DataFrame(input_data, columns=CTI_FEATURE_COLS)\n",
    "    prediction = cti_model.predict(input_df)[0]\n",
    "    predicted_class = cti_encoder.inverse_transform([prediction])[0]\n",
    "    confidence = cti_model.predict_proba(input_df)[0][int(prediction)]\n",
    "    \n",
    "    print(f\"  Test {i}: Sentiment={sample['sentiment']}, Severity={sample['severity']}\")\n",
    "    print(f\"    → Predicted: {predicted_class} ({confidence*100:.1f}% confidence)\")\n",
    "    print(f\"    → Expected: {sample['expected']}\")\n",
    "\n",
    "# Test URL model\n",
    "print(\"\\n2. URL Threat Model Test:\")\n",
    "print(\"-\" * 60)\n",
    "test_urls = [\n",
    "    ('https://www.google.com', 'Safe'),\n",
    "    ('http://secure-paypal-verify.tk', 'Phishing'),\n",
    "    ('https://github.com/user/repo', 'Safe')\n",
    "]\n",
    "\n",
    "for i, (test_url, expected) in enumerate(test_urls, 1):\n",
    "    features_dict = extract_url_features(test_url)\n",
    "    features_list = [features_dict.get(name, 0) for name in url_feature_names]\n",
    "    input_df = pd.DataFrame([features_list], columns=url_feature_names)\n",
    "    \n",
    "    prediction_proba = url_model.predict_proba(input_df)[0]\n",
    "    prediction = int(np.argmax(prediction_proba))\n",
    "    predicted_class = str(url_label_encoder[prediction])\n",
    "    confidence = prediction_proba[prediction]\n",
    "    \n",
    "    print(f\"  Test {i}: {test_url[:50]}...\")\n",
    "    print(f\"    → Predicted: {predicted_class} ({confidence*100:.1f}% confidence)\")\n",
    "    print(f\"    → Expected: {expected}\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d43221a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CELL 12: Training Complete - Summary\n",
      "============================================================\n",
      "\n",
      "✓ ALL MODELS TRAINED SUCCESSFULLY!\n",
      "\n",
      " Model Performance Summary:\n",
      "------------------------------------------------------------\n",
      "1. CTI Report Classifier:\n",
      "   - Accuracy: 21.84%\n",
      "   - Classes: 4\n",
      "   - Features: 2\n",
      "\n",
      "2. URL Threat Detector:\n",
      "   - Test Accuracy: 99.64%\n",
      "   - OOB Score: 99.63%\n",
      "   - Classes: 2\n",
      "   - Features: 55\n",
      "\n",
      "Generated Files:\n",
      "------------------------------------------------------------\n",
      "  ✓ model.pkl (0.9 KB)\n",
      "  ✓ feature_list.pkl (0.1 KB)\n",
      "  ✓ threat_encoder.pkl (0.3 KB)\n",
      "  ✓ url_model.pkl (34437.6 KB)\n",
      "  ✓ url_feature_names.pkl (0.9 KB)\n",
      "  ✓ url_label_encoder.pkl (0.2 KB)\n",
      "  ✓ url_trusted_domains.pkl (0.4 KB)\n",
      "  ✓ url_feature_importance.csv (2.0 KB)\n",
      "\n",
      "Next Steps:\n",
      "------------------------------------------------------------\n",
      "1. Close this notebook\n",
      "2. Start backend: python app.py\n",
      "3. Start frontend: python -m http.server 8000\n",
      "4. Open: http://127.0.0.1:8000/index.html\n",
      "\n",
      "============================================================\n",
      "  CTI-NLP Enhanced Analyzer - Ready for Deployment\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#Final Summary\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CELL 12: Training Complete - Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n✓ ALL MODELS TRAINED SUCCESSFULLY!\\n\")\n",
    "\n",
    "print(\" Model Performance Summary:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"1. CTI Report Classifier:\")\n",
    "print(f\"   - Accuracy: {cti_accuracy*100:.2f}%\")\n",
    "print(f\"   - Classes: {len(cti_encoder.classes_)}\")\n",
    "print(f\"   - Features: {len(CTI_FEATURE_COLS)}\")\n",
    "\n",
    "print(f\"\\n2. URL Threat Detector:\")\n",
    "print(f\"   - Test Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"   - OOB Score: {oob_score*100:.2f}%\")\n",
    "print(f\"   - Classes: {len(url_label_encoder)}\")\n",
    "print(f\"   - Features: {len(url_feature_names)}\")\n",
    "\n",
    "print(\"\\nGenerated Files:\")\n",
    "print(\"-\" * 60)\n",
    "generated_files = [\n",
    "    'model.pkl',\n",
    "    'feature_list.pkl',\n",
    "    'threat_encoder.pkl',\n",
    "    'url_model.pkl',\n",
    "    'url_feature_names.pkl',\n",
    "    'url_label_encoder.pkl',\n",
    "    'url_trusted_domains.pkl',\n",
    "    'url_feature_importance.csv'\n",
    "]\n",
    "\n",
    "for file in generated_files:\n",
    "    exists = Path(file).exists()\n",
    "    status = \"✓\" if exists else \"✗\"\n",
    "    size = Path(file).stat().st_size if exists else 0\n",
    "    size_kb = size / 1024\n",
    "    print(f\"  {status} {file} ({size_kb:.1f} KB)\")\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"1. Close this notebook\")\n",
    "print(\"2. Start backend: python app.py\")\n",
    "print(\"3. Start frontend: python -m http.server 8000\")\n",
    "print(\"4. Open: http://127.0.0.1:8000/index.html\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"  CTI-NLP Enhanced Analyzer - Ready for Deployment\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
